{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2025 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q \"google-genai==1.7.0\"\n",
    "%pip install -q google-api-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/gemini-2.0-flash-live-001\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = client.chats.create(model='gemini-2.0-flash', history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's nice to meet you, Byungryul! How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Hello! My name is Byungryul.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's something interesting about Donald Trump:\n",
      "\n",
      "While many people know him as a real estate mogul and media personality, one lesser-known fact is that **Donald Trump is the only U.S. President to have a star on the Hollywood Walk of Fame.** He received the star in 2007 for his role as the host of the reality TV show \"The Apprentice.\" This is somewhat ironic, considering his later political career and controversies surrounding Hollywood.\n",
      "\n",
      "Is there anything else you'd like to know? I can provide information on a variety of topics related to him, but I will always strive to present information objectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Can you tell me something interesting about Donald Trump?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, your name is Byungryul.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Do you remember what my name is?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = types.GenerateContentConfig(max_output_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Trump Tariff War: A Legacy of Disruption and Uncertainty\n",
      "\n",
      "Donald Trump's presidency was marked by a distinct departure from established norms of international trade, most notably through the aggressive deployment of tariffs as a tool of economic coercion and negotiation.  This \"tariff war,\" primarily targeted at China but also affecting allies like Canada and the European Union, was fueled by a complex mix of economic nationalism, a perceived unfairness in trade practices, and a desire to revitalize American manufacturing. While Trump's supporters lauded the tariffs as a necessary means to \"make America great again,\" critics condemned them as economically damaging, destabilizing the global trading system, and ultimately ineffective in achieving their stated goals.  This essay will explore the multifaceted outlook surrounding Trump's tariff war, examining its motivations, impacts, and the enduring legacy it leaves on the international economic landscape.\n",
      "\n",
      "Trump's rationale for imposing tariffs stemmed from a long-held belief that the United States had been systematically disadvantaged in global trade. He argued\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=config,\n",
    "    contents='Write a 1000 word essay on the Donald Trump\\'s tariff war outlook'\n",
    "    )\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      " -------------------------\n",
      "Seven\n",
      " -------------------------\n",
      "Seventeen\n",
      " -------------------------\n",
      "7\n",
      " -------------------------\n",
      "Seventeen\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "config = types.GenerateContentConfig(temperature=2.0)\n",
    "\n",
    "for _ in range(5):\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        config=config,\n",
    "        contents='Pick a random number... (respond in a single word)'\n",
    "    )\n",
    "\n",
    "    if response.text:\n",
    "        print(response.text, '-' * 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "확실히. 트럼프 전 대통령의 관세 정책에 대한 한 월스트리트 채권왕의 기고입니다.\n",
      "\n",
      "**트럼프의 관세: 채권 시장에 미치는 위험**\n",
      "\n",
      "도널드 트럼프 전 대통령의 관세는 경제와 미국의 건전성에 해로운 영향을 미쳤습니다. 경제학적 근거도 거의 없는 무분별한 무역 보호주의를 구현하고 있습니다.\n",
      "\n",
      "트럼프 전 대통령의 관세는 근본적으로 미국 소비자에 대한 세금입니다. 관세로 인한 추가 비용은 기업에서 소비자로 직접 전가되어 모든 상품 및 서비스 비용이 증가합니다. 이러한 비용 증가는 미국인의 구매력을 감소시켜 경제 성장을 저해합니다.\n",
      "\n",
      "또한 관세는 공급망을 혼란시키고 기업의 불확실성을 초래합니다. 관세는 기업이 생산 방식을 조정하고 새로운 공급원을 찾는 것을 어렵게 만들어 미국 산업에 피해를 줍니다. 이러한 불확실성은 투자와 성장을 지연시킵니다.\n",
      "\n",
      "그러나 가장 큰 문제는 관세가 인플레이션을 유발하는 경향이 있다는 것입니다. 이러한 추세는 관세가 기업의 생산 비용을 증가시키고, 기업이 더 높은 가격을 소비자에게 전가할 수 있기 때문입니다. 결과적으로 인플레이션 상승은 연준이 금리를 인상하도록 압력을 가하여 경제 성장을 둔화시킵니다. 더 높은 금리는 주식과 채권과 같은 자산의 가치를 감소시킵니다.\n",
      "\n",
      "게다가 관세는 보복으로 이어져 무역 전쟁으로 이어질 수 있습니다. 무역 전쟁은 경제 성장을 저해하고 국제 관계를 손상시킵니다.\n",
      "\n",
      "채권왕의 관점에서 트럼프 전 대통령의 관세는 채권 시장에 상당한 위험을 제기합니다. 관세는 인플레이션을 증가시켜 연준이 금리를 인상하도록 압력을 가할 수 있습니다. 더 높은 금리는 채권 가격 하락과 수익률 상승으로 이어질 수 있습니다. 또한 관세는 경제 성장에 대한 불확실성을 초래하여 투자자에게 위험 회피를 유발할 수 있습니다.\n",
      "\n",
      "요컨대 트럼프 전 대통령의 관세는 미국 경제에 대한 나쁜 정책입니다. 그들은 소비자들에게 세금을 부과하고, 공급망을 혼란시키고, 기업에 대한 불확실성을 조성하고, 인플레이션을 유발하고, 무역 전쟁으로 이어질 수 있습니다. 채권 시장 투자자로서 저는 트럼프 전 대통령의 관세가 경제와 시장에 미칠 수 있는 위험에 대해 우려하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "config = types.GenerateContentConfig(\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config = config,\n",
    "    contents = '당신은 월스트리트의 채권왕입니다. 도널드 트럼프의 관세 정책에 대한 짧은 신문 컬럼을 작성해 주세요'\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = types.GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"영화 리뷰를 긍정, 중립 또는 부정으로 분류하라.\n",
    "리뷰: 정말 좋았어요. 더 유명해져도 될 것 같은 영화입니다. 못봤던 영화를 재개봉 덕분에 볼 수 있어서 좋았습니다. 레이첼 맥아담스 존예... :)\n",
    "감정: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=config,\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"긍정\"\n",
    "    NEUTRAL = \"중립\"\n",
    "    NEGATIVE = \"부정\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ),\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.POSITIVE\n"
     ]
    }
   ],
   "source": [
    "print(response.parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"고객의 피자 주문을 유효한 JSON으로 구문 분석합니다:\n",
    "\n",
    "예:\n",
    "치즈와 토마토 소스 그리고 페페로니를 얹은 피자 스몰 사이즈로 주세요.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "예:\n",
    "토마토 소스에 바질 그리고 모짜렐라 치즈를 얹은 라지사이즈 피자 주문 되나요?\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "주문:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order = \"치즈와 파인애플 토핑된 라지 피자 주세요\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ),\n",
    "    contents=[prompt, order]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"내가 4살 때, 내 동료는 나보다 3배 나이가 많았다. 지금 내가 20살인데, 내 동료의 나이는 얼마인가? 답변만 알려주세요\"\"\"\n",
    "\n",
    "# prompt = \"\"\"내가 4살 때, 내 동료는 나보다 3배 나이가 많았다. 지금 내가 20살인데, 내 동료의 나이는 얼마인가? 단계별로 생각해 보자.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52세\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "사고, 행동, 관찰 단계를 번갈아 가며 수행하는 질의응답 과제를 해결하세요. \n",
    "사고는 현재 상황에 대해 추론할 수 있고, \n",
    "관찰은 행동의 결과에서 관련 정보를 이해하는 것이며, \n",
    "행동은 다음 세 가지 유형 중 하나일 수 있습니다.\n",
    "  1. <search>항목</search>, 위키백과에서 정확한 항목을 검색하여 첫 번째 문단이 있으면 해당 항목을 반환합니다. 없으면 유사한 항목을 검색하여 해당 주제의 정보를 검색해 볼 수 있습니다.\n",
    "  2. <lookup>키워드</lookup>, 현재 맥락에서 키워드가 포함된 다음 문장을 반환합니다. 이 기능은 정확히 일치하는 항목만 검색하므로 검색어를 짧게 작성하세요.\n",
    "  3. <finish>대답</finish>, 그러면 대답이 반환되고 작업이 완료됩니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"\"\"질문\n",
    "음악가이자 풍자작가인 앨리 고어츠는 \"심슨 가족\"의 등장인물인 밀하우스에 대한 노래를 썼는데, 맷 그로닝은 그의 이름을 누구의 이름에서 따왔을까요?\n",
    "\n",
    "사고 1\n",
    "질문은 \"심슨 가족\"의 등장인물 밀하우스의 이름이 누구의 이름을 따서 지어졌는지로 단순화됩니다. 밀하우스를 검색해서 누구의 이름을 따서 지었는지 찾으면 됩니다.\n",
    "\n",
    "행동 1\n",
    "<search>밀하우스</search>\n",
    "\n",
    "관찰 1\n",
    "밀하우스 무솔리니 반 하우튼은 패멀라 헤이든이 목소리를 맡고 맷 그로닝이 창조한 폭스 애니메이션 텔레비전 시리즈 심슨 가족에 반복적으로 등장하는 캐릭터입니다.\n",
    "\n",
    "사고 2\n",
    "해당 문단에서는 밀하우스라는 이름이 누구의 이름을 따서 지어졌는지 언급하지 않습니다. 아마 \"named from\"을 찾아볼 수 있을 겁니다.\n",
    "\n",
    "행동 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "관찰 2\n",
    "밀하우스라는 이름은 미국 대통령 리처드 닉슨의 이름을 따서 지어졌는데, 그의 중간 이름은 밀하우스였다.\n",
    "\n",
    "사고 3\n",
    "밀하우스는 미국 대통령 리처드 닉슨의 이름을 따서 지어졌으므로 답은 리처드 닉슨입니다.\n",
    "\n",
    "행동 3\n",
    "<finish>리처드 닉슨</finish>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2 = \"\"\"\n",
    "질문\n",
    "콜로라도 조산운동의 동쪽 부분이 확장되는 지역의 고도 범위는 어떻게 됩니까?\n",
    "\n",
    "생각 1\n",
    "콜로라도 조산운동을 검색하여 콜로라도 조산운동의 동쪽 부분이 확장되는 지역을 찾은 다음, 해당 지역의 고도 범위를 구해야 합니다.\n",
    "\n",
    "활동 1\n",
    "<search>콜로라도 조산운동</search>\n",
    "\n",
    "관찰 1\n",
    "콜로라도 조산운동은 콜로라도와 주변 지역에서 발생한 조산운동(조산운동)의 한 사례였습니다.\n",
    "\n",
    "생각 2\n",
    "동부 영역에 대한 언급이 없으므로, 동부 영역을 찾아야 합니다.\n",
    "\n",
    "활동 2\n",
    "<lookup>동부 영역</lookup>\n",
    "\n",
    "관찰 2\n",
    "동부 영역은 고원 지대로 확장되며, 중부 평원 조산운동이라고 합니다.\n",
    "\n",
    "생각 3\n",
    "콜로라도 조산운동의 동쪽 영역은 고원 지대로 확장됩니다. 따라서 고원 지대를 검색하여 고도 범위를 구해야 합니다.\n",
    "\n",
    "액션 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "관찰 3\n",
    "High Plains는 두 개의 뚜렷한 육지 지역 중 하나를 가리킵니다.\n",
    "\n",
    "생각 4\n",
    "High Plains(미국)를 대신 검색해야 합니다.\n",
    "\n",
    "액션 4\n",
    "<search>High Plains(미국)</search>\n",
    "\n",
    "관찰 4\n",
    "High Plains는 Great Plains의 하위 지역입니다. 동쪽에서 서쪽으로 갈수록 High Plains의 고도는 약 1,800~7,000피트(550~2,130m)입니다.\n",
    "\n",
    "생각 5\n",
    "High Plains의 고도는 약 1,800~7,000피트이므로 정답은 1,800~7,000피트입니다.\n",
    "\n",
    "액션 5\n",
    "<finish>1,800~7,000피트</finish>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"질문\n",
    "트랜스포머 NLP 논문에 등재된 가장 어린 저자는 누구였습니까?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생각 1\n",
      "트랜스포머 NLP 논문의 이름을 검색하여 자세한 내용을 찾아야 합니다. 그런 다음 논문에 등재된 가장 어린 저자를 찾습니다.\n",
      "\n",
      "행동 1\n",
      "<search>트랜스포머 NLP 논문</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "react_chat = client.chats.create(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        stop_sequences=[\"\\n관찰\"],\n",
    "        system_instruction=instruction + example1 + example2,\n",
    "    )\n",
    ")\n",
    "\n",
    "response = react_chat.send_message(question)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사고 2\n",
      "논문에 등재된 저자는 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin입니다. 그들이 몇 살인지 추가 정보를 찾아야 합니다.\n",
      "\n",
      "행동 2\n",
      "<search>Ashish Vaswani 나이</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"관찰 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "본 연구에서는 재귀와 합성곱을 전혀 사용하지 않고 주의 메커니즘에만 기반한 새로운 단순 네트워크 아키텍처인 트랜스포머를 제안합니다.\n",
    "\"\"\"\n",
    "\n",
    "response = react_chat.send_message(observation)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content_stream(\n",
    "    model='gemini-2.0-flash-thinking-exp',\n",
    "    contents='트랜스포머 NLP 논문에 등재된 가장 어린 저자는 누구였습니까?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트랜스포머 NLP 논문, 즉 \"Attention is All You Need\" 논문에 등재된 가장 어린 저자는 **에이단 N. 고메즈 (Aidan N. Gomez)**입니다.\n",
      "\n",
      "에이단 N. 고메즈는 당시 토론토 대학교의 학부생 인턴으로 Google Brain 팀에 합류하여 이 논문에 참여했습니다. 다른 저자들은 대부분 박사 학위 소지자이거나 이미 경력을 쌓은 연구자들이었던 반면, 에이단 고메즈는 학부생 신분으로 이 중요한 논문에 기여했기 때문에 가장 어린 저자로 여겨집니다.\n",
      "\n",
      "따라서 트랜스포머 논문에서 가장 어린 저자는 **에이단 N. 고메즈**입니다."
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    print(chunk.text, end='')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "트랜스포머 NLP 논문, 즉 \"Attention is All You Need\" 논문에 등재된 가장 어린 저자는 **에이단 N. 고메즈 (Aidan N. Gomez)**입니다.\n",
       "\n",
       "에이단 N. 고메즈는 당시 토론토 대학교의 학부생 인턴으로 Google Brain 팀에 합류하여 이 논문에 참여했습니다. 다른 저자들은 대부분 박사 학위 소지자이거나 이미 경력을 쌓은 연구자들이었던 반면, 에이단 고메즈는 학부생 신분으로 이 중요한 논문에 기여했기 때문에 가장 어린 저자로 여겨집니다.\n",
       "\n",
       "따라서 트랜스포머 논문에서 가장 어린 저자는 **에이단 N. 고메즈**입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"숫자의 팩토리얼을 계산하는 Python 함수를 작성하시오. 설명은 없고 코드만 제공하시오.\"\"\"\n",
    "\n",
    "# prompt = \"\"\"숫자의 팩토리얼을 계산하는 Python 함수를 작성하세요. 설명은 없고 코드만 제공하세요.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ),\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"처음 14개의 홀수 소수를 생성한 다음, 그 합을 계산합니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[types.Tool(codeExecution=types.ToolCodeExecution())]\n",
    "    ),\n",
    "    contents=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '알겠습니다. 먼저 처음 14개의 홀수 소수를 생성한 후 그 합을 계산하겠습니다.\\n'\n",
      "         '\\n'\n",
      "         '소수는 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, '\n",
      "         '61, 67, 71, 73, 79, 83, 89, 97 등과 같이 1과 자기 자신으로만 나누어지는 1보다 큰 자연수입니다. '\n",
      "         '홀수 소수는 2를 제외한 모든 소수입니다.\\n'\n",
      "         '\\n'\n",
      "         '처음 14개의 홀수 소수는 다음과 같습니다: 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, '\n",
      "         '41, 43, 47\\n'\n",
      "         '\\n'\n",
      "         '이제 이 숫자들의 합을 계산하겠습니다.\\n'}\n",
      "-----\n",
      "{'executable_code': {'code': 'import numpy as np\\n'\n",
      "                             '\\n'\n",
      "                             'primes = np.array([3, 5, 7, 11, 13, 17, 19, 23, '\n",
      "                             '29, 31, 37, 41, 43, 47])\\n'\n",
      "                             'sum_of_primes = np.sum(primes)\\n'\n",
      "                             'print(sum_of_primes)\\n',\n",
      "                     'language': 'PYTHON'}}\n",
      "-----\n",
      "{'code_execution_result': {'outcome': 'OUTCOME_OK', 'output': '326\\n'}}\n",
      "-----\n",
      "{'text': '따라서 처음 14개의 홀수 소수의 합은 326입니다.\\n'}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    pprint(part.to_json_dict())\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "알겠습니다. 먼저 처음 14개의 홀수 소수를 생성한 후 그 합을 계산하겠습니다.\n",
       "\n",
       "소수는 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97 등과 같이 1과 자기 자신으로만 나누어지는 1보다 큰 자연수입니다. 홀수 소수는 2를 제외한 모든 소수입니다.\n",
       "\n",
       "처음 14개의 홀수 소수는 다음과 같습니다: 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47\n",
       "\n",
       "이제 이 숫자들의 합을 계산하겠습니다.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import numpy as np\n",
       "\n",
       "primes = np.array([3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47])\n",
       "sum_of_primes = np.sum(primes)\n",
       "print(sum_of_primes)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "326\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "따라서 처음 14개의 홀수 소수의 합은 326입니다.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text:\n",
    "        display(Markdown(part.text))\n",
    "    elif part.executable_code:\n",
    "        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
    "    elif part.code_execution_result:\n",
    "        if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
    "            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n",
    "\n",
    "        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "이 파일의 기능을 개략적으로 설명해 주세요. 이 파일은 무엇이고, 왜 사용해야 하나요?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "이 파일은 `bash-git-prompt`라는 도구의 핵심 스크립트입니다. 이 도구는 Bash 또는 Zsh 셸 프롬프트에 Git 저장소의 상태를 표시하여 개발자가 작업 중인 저장소의 상태를 빠르게 파악할 수 있도록 도와줍니다.\n",
       "\n",
       "**핵심 기능:**\n",
       "\n",
       "*   **Git 정보 표시:** 현재 브랜치, 커밋 상태, 변경 사항, 추적 여부 등 Git 저장소의 다양한 정보를 셸 프롬프트에 표시합니다.\n",
       "*   **사용자 정의 가능:** 색상, 기호, 정보 표시 방식 등을 사용자 정의할 수 있는 다양한 옵션을 제공합니다. 테마를 변경하거나 자신만의 테마를 만들 수도 있습니다.\n",
       "*   **성능:** 비동기 실행 방식을 사용하여 프롬프트가 느려지는 것을 최소화합니다.\n",
       "*   **가상 환경 지원:**  `virtualenv`, `conda`, `node virtual env` 등 가상 환경 정보 표시도 지원합니다.\n",
       "\n",
       "**왜 사용해야 할까요?**\n",
       "\n",
       "*   **생산성 향상:** Git 저장소의 상태를 시각적으로 빠르게 확인할 수 있어 개발 워크플로우를 개선하고 오류를 줄일 수 있습니다.\n",
       "*   **정보:** 작업 중인 브랜치, 변경 사항, 커밋 상태를 항상 파악할 수 있습니다.\n",
       "*   **개인화:** 자신에게 맞는 스타일로 프롬프트를 꾸밀 수 있습니다.\n",
       "*   **편의성:** Git 명령어를 실행하지 않고도 저장소 상태를 즉시 확인할 수 있습니다.\n",
       "\n",
       "**사용 방법:**\n",
       "\n",
       "1.  `bash-git-prompt`를 다운로드하여 설치합니다. (일반적으로 Git 저장소로 관리됨)\n",
       "2.  `.bashrc` 또는 `.zshrc` 파일에 해당 스크립트를 소싱하는 코드를 추가합니다 (예: `source /path/to/git-prompt.sh`).\n",
       "3.  터미널을 재시작하거나 해당 설정 파일을 다시 로드합니다 (예: `source ~/.bashrc`).\n",
       "\n",
       "**파일 내용 분석:**\n",
       "\n",
       "*   **함수 정의:** `async_run`, `set_git_prompt_dir`, `echoc`, `get_theme`, `git_prompt_config`, `updatePrompt` 등 다양한 함수들이 정의되어 있습니다. 각 함수는 Git 정보 가져오기, 색상 설정, 프롬프트 구성 등 특정 작업을 수행합니다.\n",
       "*   **설정 변수:** `GIT_PROMPT_THEME`, `GIT_PROMPT_START`, `GIT_PROMPT_END` 등 다양한 설정 변수들이 사용됩니다. 이러한 변수를 통해 프롬프트의 모양과 동작을 사용자 정의할 수 있습니다.\n",
       "*   **색상 설정:** `prompt-colors.sh` 파일을 로드하여 색상 변수를 정의하고 프롬프트에 적용합니다.\n",
       "*   **상태 정보 표시:** `git status` 명령어를 실행하여 저장소의 상태를 파악하고, 결과를 프롬프트에 표시합니다.\n",
       "*   **가상 환경 지원:** 가상 환경 관련 변수를 검사하고, 프롬프트에 가상 환경 정보를 추가합니다.\n",
       "*   **프롬프트 업데이트:** `setGitPrompt` 함수는 `PROMPT_COMMAND`에 추가되어 셸 프롬프트를 업데이트합니다.\n",
       "\n",
       "요약하자면, 이 파일은 셸 프롬프트에 Git 정보를 표시하는 데 필요한 핵심 로직을 담고 있으며, 사용자가 원하는 대로 프롬프트를 사용자 정의할 수 있도록 다양한 설정 옵션을 제공합니다.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
